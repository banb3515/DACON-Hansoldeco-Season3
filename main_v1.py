import builtins
import os
import json
import torch
import logging
import pandas as pd
import numpy as np

from dotenv import load_dotenv

from typing import Literal

from string import Template

from datetime import datetime

from tqdm import tqdm

from vllm import LLM, SamplingParams

from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

from sentence_transformers import SentenceTransformer


# vLLM ë¡œê·¸ ë¹„í™œì„±í™”
logger = logging.getLogger("vllm")
logger.setLevel(logging.ERROR)


#region 0. Utils

_log_file = None

def print(log: str, log_dir: str = "./logs") -> None:
    """Print log with timestamp

    Args:
        log (str): log message
    """
    
    global _log_file
    
    if _log_file is None:
        os.makedirs(log_dir, exist_ok=True)
        log_filename = datetime.now().strftime("%Y%m%d_%H%M%S") + ".log"
        _log_file = open(os.path.join(log_dir, log_filename), "w", encoding="utf-8")
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    
    builtins.print(f"\033[36m[\033[1m{timestamp}\033[0m\033[36m]\033[0m {log}")
    
    _log_file.write(f"[{timestamp}] {log}\n")
    _log_file.flush()

#endregion

#region 1. Data Processing

def load_data(data_dir: str = "./data") -> tuple[pd.DataFrame, pd.DataFrame]:
    """Load train and test data

    Returns:
        tuple[pd.DataFrame, pd.DataFrame]: (train data, test data)
    """

    train = pd.read_csv(os.path.join(data_dir, "train.csv"), encoding="utf-8-sig")
    test = pd.read_csv(os.path.join(data_dir, "test.csv"), encoding="utf-8-sig")
    return train, test

def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    """Preprocess data

    Args:
        df (pd.DataFrame): raw data

    Returns:
        pd.DataFrame: preprocessed data
    """
    
    df["ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)"] = df["ê³µì‚¬ì¢…ë¥˜"].str.split(" / ").str[0]
    df["ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)"] = df["ê³µì‚¬ì¢…ë¥˜"].str.split(" / ").str[1]
    df["ê³µì¢…(ëŒ€ë¶„ë¥˜)"] = df["ê³µì¢…"].str.split(" > ").str[0]
    df["ê³µì¢…(ì¤‘ë¶„ë¥˜)"] = df["ê³µì¢…"].str.split(" > ").str[1]
    df["ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)"] = df["ì‚¬ê³ ê°ì²´"].str.split(" > ").str[0]
    df["ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)"] = df["ì‚¬ê³ ê°ì²´"].str.split(" > ").str[1]
    return df

def create_combined_data(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:
    """Create combined data

    Args:
        df (pd.DataFrame): raw data
        is_train (bool, optional): whether the data is train data. Defaults to True.

    Returns:
        pd.DataFrame: combined data
    """
    
    combined_data = df.apply(
        lambda row: {
            "question": (
                f"ê³µì‚¬ì¢…ë¥˜ ëŒ€ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)']}' ê³µì‚¬ ì¤‘ "
                f"ê³µì¢… ëŒ€ë¶„ë¥˜ '{row['ê³µì¢…(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì¢…(ì¤‘ë¶„ë¥˜)']}' ì‘ì—…ì—ì„œ "
                f"ì‚¬ê³ ê°ì²´ '{row['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)']}'(ì¤‘ë¶„ë¥˜: '{row['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)']}')ì™€ ê´€ë ¨ëœ ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
                f"ì‘ì—… í”„ë¡œì„¸ìŠ¤ëŠ” '{row['ì‘ì—…í”„ë¡œì„¸ìŠ¤']}'ì´ë©°, ì‚¬ê³  ì›ì¸ì€ '{row['ì‚¬ê³ ì›ì¸']}'ì…ë‹ˆë‹¤. "
                f"ì¬ë°œ ë°©ì§€ ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ),
            **({"answer": row["ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš"]} if is_train else {})
        },
        axis=1
    )
    return pd.DataFrame(list(combined_data))

#endregion

#region 2. Model Loading

def load_embedding_model(model_name: str, **kwargs) -> Embeddings:
    """Load embedding model

    Args:
        model_name (str): model name
        **kwargs: additional parameters for embedding model

    Returns:
        Embeddings: loaded embedding model
    """
    
    return HuggingFaceEmbeddings(
        model_name=model_name,
        **kwargs
    )

def load_llm_model(model_name: str, **kwargs) -> LLM:
    """Load LLM model

    Args:
        model_name (str): model name
        **kwargs: additional parameters for vLLM

    Returns:
        LLM: loaded chat model
    """
    
    return LLM(
        model=model_name,
        tensor_parallel_size=torch.cuda.device_count(),
        disable_async_output_proc=True,
        disable_custom_all_reduce=True,
        **kwargs
    )

#endregion

#region 3. Main Tasks

def create_train_documents(data: pd.DataFrame) -> list[Document]:
    """Create train documents

    Args:
        data (pd.DataFrame): train dataset

    Returns:
        list[Document]: train documents
    """
    
    train_questions = data["question"].tolist()
    train_answers = data["answer"].tolist()
    
    return [
        Document(f"""\
<question>{question}</question>
<answer>{answer}</answer>\
""")
        for question, answer in zip(train_questions, train_answers)
    ]

def create_retriever(
    documents: list[Document],
    embedding: Embeddings,
    cache_path: str,
    path: str,
    index_name: str,
    search_type: Literal["similarity", "mmr"] = "similarity",
    k: int = 10,
    lambda_mult: float = 0.5
) -> VectorStoreRetriever:
    """Create retriever

    Args:
        documents (list[Document]): documents
        embedding (Embeddings): embedding model
        cache_path (str): cache path
        path (str): path
        index_name (str): index name
        search_type (Literal["similarity", "mmr"], optional): search type. Defaults to "similarity".
        k (int, optional): k. Defaults to 10.
        lambda_mult (float, optional): lambda mult. Defaults to 0.5.

    Returns:
        VectorStoreRetriever: retriever
    """
    
    embedding_store = LocalFileStore(cache_path)
    cached_embedder = CacheBackedEmbeddings.from_bytes_store(embedding, embedding_store, namespace=f"{index_name}_embed-")
    
    embedding_texts = []
    embedding_datas = []
    metadatas = []
    
    for doc in tqdm(documents, desc="Embedding documents"):
        embedding_vector = cached_embedder.embed_documents([doc.page_content])
        embedding_texts.append(doc.page_content)
        embedding_datas.extend(embedding_vector)
        metadatas.append(doc.metadata)
    
    vectorstore = FAISS.from_embeddings(zip(embedding_texts, embedding_datas), embedding, metadatas=metadatas)
    vectorstore.save_local(path, index_name)
    
    search_kwargs = {"k": k}
    if search_type == "mmr":
        search_kwargs.update({
            "fetch_k": k * 2,
            "lambda_mult": lambda_mult
        })
    
    return vectorstore.as_retriever(
        search_type=search_type,
        **search_kwargs
    )

def exec_test(
    retriever: VectorStoreRetriever,
    reasoning_model: LLM,
    chat_model: LLM,
    reasoning_sampling_params: SamplingParams,
    chat_sampling_params: SamplingParams,
    data: pd.DataFrame
) -> list[str]:
    """Execute test

    Args:
        retriever (VectorStoreRetriever): retriever
        reasoning_model (LLM): reasoning model
        chat_model (LLM): chat model
        reasoning_sampling_params (SamplingParams): reasoning sampling parameters
        chat_sampling_params (SamplingParams): chat sampling parameters
        data (pd.DataFrame): test dataset

    Returns:
        list[str]: results
    """
    
    REASONING_SYSTEM_PROMPT = Template("""\
<role>
You are a construction safety expert.
</role>
<rules>
- Always respond in English.
- Summarize only the core content of the answer concisely.
- Never include introductions, background, or additional explanations.
- Do not include phrases such as "We suggest taking the following actions:".
- Clearly list safety measures and action plans.
- Provide the answer in a single sentence as presented in the provided examples.
- Base your answer on the provided examples.
</rules>
<examples>
${examples}
</examples>\
""")
    CHAT_SYSTEM_PROMPT = Template("""\
<role>
ë‹¹ì‹ ì€ ê±´ì„¤ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
</role>
<rules>
- í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
- ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•µì‹¬ ë‚´ìš©ë§Œ ìš”ì•½í•˜ì—¬ ê°„ëµí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ì„œë¡ , ë°°ê²½ ì„¤ëª… ë˜ëŠ” ì¶”ê°€ ì„¤ëª…ì„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ë‹¤ìŒê³¼ ê°™ì€ ì¡°ì¹˜ë¥¼ ì·¨í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤: ì™€ ê°™ì€ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ì•ˆì „ ëŒ€ì±…ê³¼ ì¡°ì¹˜ ê³„íšì„ ëª…í™•í•˜ê²Œ ë‚˜ì—´í•˜ì„¸ìš”.
- ì œê³µëœ ì˜ˆì‹œ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
- <think> íƒœê·¸ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì œê³µëœ ì˜ˆì‹œ ìë£Œì™€ ê°™ì´ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
- ë‹µë³€ ëì— "í•œë‹¤.", "í•©ë‹ˆë‹¤" ë“± ì¡°ë™ì‚¬ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
</rules>
<examples>
${examples}
</examples>
<think>
${think}
</think>
""")
    
    
    def _inference(question: str):
        contexts = retriever.invoke(question)
        examples = [
            f"<example>\n{context.page_content}\n</example>"
            for context in contexts
        ]
        
        # TODO: Reranking ëª¨ë¸ ì ìš©
        
        # Reasoning
        reasoning_prompt = REASONING_SYSTEM_PROMPT.substitute(examples="\n".join(examples))
        reasoning_output = reasoning_model.chat(
            [
                {"role": "system", "content": reasoning_prompt},
                {"role": "user", "content": question}
            ],
            sampling_params=reasoning_sampling_params,
            use_tqdm=False
        )
        reasoning_text = reasoning_output[0].outputs[0].text.strip()
        
        print(f"# Reasoning:\n{reasoning_text}")
        
        # Chat
        chat_prompt = CHAT_SYSTEM_PROMPT.substitute(examples="\n".join(examples), think=reasoning_text)
        chat_output = chat_model.chat(
            [
                {"role": "system", "content": chat_prompt},
                {"role": "user", "content": question},
                {"role": "assistant", "content": f"ì‘ì—…ì „ ì•ˆì „êµìœ¡ ì‹¤ì‹œì™€ ì•ˆì „ê´€ë¦¬ì ì•ˆì „ì ê²€ ì‹¤ì‹œë¥¼ í†µí•œ ì¬ë°œ ë°©ì§€ ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íš: "}
            ],
            sampling_params=chat_sampling_params,
            use_tqdm=False
        )
        result_text = chat_output[0].outputs[0].text.strip()
        
        print(f"# Result:\n{result_text}")
        
        return result_text
    
    questions = data["question"].tolist()
    
    results = []
    for question in tqdm(questions, desc="Inferring ..."):
        results.append(_inference(question))
    
    return results

def create_result_embeddings(results: list[str], batch_size: int = 64, model_name: str = "jhgan/ko-sbert-sts") -> np.ndarray:
    """Create result embeddings

    Args:
        results (list[str]): results
        batch_size (int, optional): batch size. Defaults to 64.
        model_name (str, optional): model name. Defaults to "jhgan/ko-sbert-sts".

    Returns:
        np.ndarray: result embeddings
    """
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    embedding = SentenceTransformer(model_name).to(device)
    
    return embedding.encode(
        results,
        batch_size=batch_size,
        show_progress_bar=True,
        device=device
    )

def save_results(results: list[str], embeddings: np.ndarray, data_dir: str = "./data", submissions_dir: str = "./submissions") -> None:
    """Save results

    Args:
        results (list[str]): results
        embeddings (np.ndarray): embeddings
        data_dir (str, optional): data directory. Defaults to "./data".
        submissions_dir (str, optional): submissions directory. Defaults to "./submissions".
    """
    
    submission = pd.read_csv(os.path.join(data_dir, "sample_submission.csv"), encoding="utf-8-sig")
    submission.iloc[:,1] = results
    submission.iloc[:,2:] = embeddings
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    os.makedirs(submissions_dir, exist_ok=True)
    filename = f"{submissions_dir}/submission_{timestamp}.csv"
    submission.to_csv(filename, index=False, encoding="utf-8-sig")

#endregion


if __name__ == "__main__":
    print("ğŸš€ Start !")
    
    # Environment Variables Loading
    load_dotenv(override=True)
    
    DATA_DIR = os.getenv("DATA_DIR", "./data")
    SUBMISSIONS_DIR = os.getenv("SUBMISSIONS_DIR", "./submissions")
    CACHE_PATH = os.getenv("CACHE_PATH", "./cache")
    FAISS_PATH = os.getenv("FAISS_PATH", "./faiss")
    FAISS_INDEX_NAME = os.getenv("FAISS_INDEX_NAME", "dacon")
    
    EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
    SEARCH_OPTIONS = json.loads(os.getenv("SEARCH_OPTIONS"))
    
    REASONING_MODEL_NAME = os.getenv("REASONING_MODEL_NAME")
    REASONING_MODEL_OPTIONS = json.loads(os.getenv("REASONING_MODEL_OPTIONS", "{}"))
    REASONING_MODEL_SAMPLING_PARAMS = json.loads(os.getenv("REASONING_MODEL_SAMPLING_PARAMS", "{}"))
    
    CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
    CHAT_MODEL_OPTIONS = json.loads(os.getenv("CHAT_MODEL_OPTIONS", "{}"))
    CHAT_MODEL_SAMPLING_PARAMS = json.loads(os.getenv("CHAT_MODEL_SAMPLING_PARAMS", "{}"))
    
    # Data Loading
    print("ğŸ—‚ï¸ Data loading ...")
    train_data, test_data = load_data(DATA_DIR)
    
    # Data Preprocessing
    print("ğŸ”„ Preprocessing train data ...")
    train_data = preprocess_data(train_data)
    print("ğŸ”„ Preprocessing test data ...")
    test_data = preprocess_data(test_data)
    
    # Data Combination
    print("ğŸ”— Create combined data ...")
    combined_train_data = create_combined_data(train_data, is_train=True)
    combined_test_data = create_combined_data(test_data, is_train=False)
    
    # Create Embedding Model
    print("ğŸ“¥ Loading embedding model ...")
    embedding = load_embedding_model(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    # Create Train Documents
    print("ğŸ“„ Create train documents ...")
    train_documents = create_train_documents(combined_train_data)
    
    # Create Retriever
    print("ğŸ” Create retriever ...")
    retriever = create_retriever(
        train_documents,
        embedding,
        CACHE_PATH,
        FAISS_PATH,
        FAISS_INDEX_NAME,
        **SEARCH_OPTIONS
    )
    
    # Reasoning Model Loading
    print("ğŸ§  Loading reasoning model ...")
    reasoning_model = load_llm_model(
        model_name=REASONING_MODEL_NAME,
        **REASONING_MODEL_OPTIONS,
    )
    reasoning_sampling_params = SamplingParams(**REASONING_MODEL_SAMPLING_PARAMS)
    
    # Chat Model Loading
    print("ğŸ’¬ Loading chat model ...")
    chat_model = load_llm_model(
        model_name=CHAT_MODEL_NAME,
        **CHAT_MODEL_OPTIONS
    )
    chat_sampling_params = SamplingParams(**CHAT_MODEL_SAMPLING_PARAMS)
    
    # Execute Test
    print("ğŸ§ª Testing execution ...")
    results = exec_test(
        retriever,
        reasoning_model,
        chat_model,
        reasoning_sampling_params,
        chat_sampling_params,
        combined_test_data,
    )
    
    # Create Result Embeddings
    print("ğŸ”¢ Create result embeddings ...")
    result_embeddings = create_result_embeddings(results)
    
    # Save Results
    print("ğŸ’¾ Save results ...")
    save_results(results, result_embeddings, DATA_DIR, SUBMISSIONS_DIR)
    
    print("ğŸ End !")
